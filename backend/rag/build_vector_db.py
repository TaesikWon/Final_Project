# backend/rag/build_vector_db.py

import os
import chromadb
from sentence_transformers import SentenceTransformer

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VECTOR_DB_PATH = os.path.join(BASE_DIR, "vector_db")

print(f"ğŸ“Œ Vector DB Path: {VECTOR_DB_PATH}")

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

client = chromadb.PersistentClient(path=VECTOR_DB_PATH)

collection = client.get_or_create_collection(
    name="facility_rules",
    metadata={"hnsw:space": "cosine"}
)

documents = [
    {
        "doc_id": "rule_01",
        "type": "rule",
        "category": "sports",
        "title": "ì²´ìœ¡Â·ìŠ¤í¬ì¸ ì‹œì„¤ ì ‘ê·¼ì„± ê¸°ì¤€",
        "content": "ì²´ìœ¡Â·ìŠ¤í¬ì¸ ì‹œì„¤ì€ ìƒí™œSOC ê¸°ì¤€ì— ë”°ë¼ 500m~1km ë„ë³´ ìƒí™œê¶Œ ë‚´ ì ‘ê·¼ì„±ì´ ì ì • ê±°ë¦¬ë¡œ í™œìš©ëœë‹¤.",
        "tags": ["sports", "facility", "distance"]
    },
    {
        "doc_id": "rule_02",
        "type": "rule",
        "category": "shopping",
        "title": "ì‡¼í•‘ ë° ê·¼ë¦°ìƒê°€ ì ‘ê·¼ì„± ê¸°ì¤€",
        "content": "ì‡¼í•‘ ë° ê·¼ë¦°ìƒê°€ ì‹œì„¤ì€ ê·¼ë¦°ìƒí™œê¶Œ ê¸°ì¤€ì—ì„œ 300m~600m ë„ë³´ ì ‘ê·¼ì´ ì¼ë°˜ì ì¸ ìƒí™œ í¸ì˜ ë²”ìœ„ë¡œ ì‚¬ìš©ëœë‹¤.",
        "tags": ["shopping", "mart", "distance"]
    },
    {
        "doc_id": "rule_03",
        "type": "rule",
        "category": "medical",
        "title": "ì˜ë£Œê¸°ê´€ ì ‘ê·¼ì„± ê¸°ì¤€",
        "content": "1ì°¨ ì˜ë£Œê¸°ê´€ê³¼ ë³‘ì›ì€ ì§€ì—­ ì˜ë£Œ ì ‘ê·¼ì„± í‰ê°€ ì‹œ ë„ë³´ 500m ìƒí™œê¶Œì´ ì ì • ê±°ë¦¬ë¡œ í™œìš©ëœë‹¤.",
        "tags": ["medical", "hospital", "distance"]
    },
    {
        "doc_id": "rule_04",
        "type": "rule",
        "category": "mart_market",
        "title": "ë§ˆíŠ¸Â·ì‹œì¥ ìƒí™œê¶Œ ê¸°ì¤€",
        "content": "ë§ˆíŠ¸ì™€ ì‹œì¥ì€ ë„ì‹œ ìƒí™œê¶Œ ì†Œë¹„í–‰íƒœ ê¸°ì¤€ì—ì„œ 300m~700m ì ‘ê·¼ì„±ì´ ì¥ë³´ê¸° ìƒí™œê¶Œìœ¼ë¡œ ê°„ì£¼ëœë‹¤.",
        "tags": ["mart", "market", "distance"]
    },
    {
        "doc_id": "rule_05",
        "type": "rule",
        "category": "restaurant",
        "title": "ì‹ë‹¹Â·ì™¸ì‹ì‹œì„¤ ì ‘ê·¼ì„± ê¸°ì¤€",
        "content": "ì‹ë‹¹ê³¼ ì™¸ì‹ì‹œì„¤ì€ ê·¼ë¦°ìƒí™œê¶Œ ê³„íš ê¸°ì¤€ì—ì„œ 200m~400m ë„ë³´ ìƒí™œê¶Œì´ ì¼ë°˜ì ì¸ ì ‘ê·¼ ê±°ë¦¬ë¡œ ì‚¬ìš©ëœë‹¤.",
        "tags": ["food", "restaurant", "distance"]
    }
]


def chunk_text(text, chunk_size=180):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]


chunk_documents = []
for doc in documents:
    chunks = chunk_text(doc["content"])
    for idx, chunk in enumerate(chunks):
        chunk_documents.append({
            "doc_id": f"{doc['doc_id']}_chunk_{idx+1}",
            "parent_id": doc["doc_id"],
            "type": doc["type"],
            "category": doc["category"],
            "title": doc["title"],
            "content": chunk,
            # ğŸ”¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì—ëŸ¬ í•´ê²°)
            "tags": ",".join(doc["tags"])
        })


contents = [d["content"] for d in chunk_documents]
ids = [d["doc_id"] for d in chunk_documents]
metadatas = [
    {
        "parent_id": d["parent_id"],
        "type": d["type"],
        "category": d["category"],
        "title": d["title"],
        # ğŸ”¥ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ë¬¸ìì—´ ì €ì¥
        "tags": d["tags"]
    }
    for d in chunk_documents
]

embeddings = model.encode(contents).tolist()

collection.add(
    documents=contents,
    ids=ids,
    metadatas=metadatas,
    embeddings=embeddings
)

print("âœ” RAG ë²¡í„°DB ìƒì„± ì™„ë£Œ!")
print(f"ì´ ì²­í¬ ìˆ˜: {len(chunk_documents)}")
print(f"â¡ ì €ì¥ ê²½ë¡œ: {VECTOR_DB_PATH}")
