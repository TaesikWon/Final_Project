# backend/llm_parser.py

from dotenv import load_dotenv
load_dotenv()

import os
import json
from openai import OpenAI

# ìˆ˜ì •ëœ import (ì ˆëŒ€ê²½ë¡œ)
from backend.utils.rag_service import RAGService

api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    print("âŒ ERROR: OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    client = None
else:
    client = OpenAI(api_key=api_key)


class LLMParser:

    def __init__(self):
        print("ğŸ“Œ RAG ê¸°ë°˜ GPT Parser Loaded")
        self.rag = RAGService()

    def parse_to_conditions(self, text: str) -> dict:

        if client is None:
            return {"error": "GPT ì‚¬ìš© ë¶ˆê°€"}

        rag_rules = self.rag.search(text)
        rag_prompt = "\n".join([f"- {rule}" for rule in rag_rules])

        prompt = f"""
ë„ˆëŠ” JSON ì¡°ê±´ ìƒì„± íŒŒì„œì´ë‹¤.

RAG ê·œì¹™:
{rag_prompt}

ì…ë ¥ ë¬¸ì¥:
{text}

JSONë§Œ ì¶œë ¥í•˜ë¼.
"""

        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "JSONë§Œ ìƒì„±"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1
        )

        raw = response.choices[0].message.content.strip()

        try:
            return json.loads(raw)
        except:
            return {"error": "JSON íŒŒì‹± ì‹¤íŒ¨", "raw_output": raw}
