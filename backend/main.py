# backend/main.py

import os
import torch
import torch.nn as nn
import pandas as pd
from fastapi import FastAPI, APIRouter
from pydantic import BaseModel

from dotenv import load_dotenv
from openai import OpenAI
from anthropic import Anthropic
from kobert_transformers import get_tokenizer, get_kobert_model

from backend.llm_parser import LLMParser
from backend.recommender import Recommender
from backend.utils.rag_service import RAGService

load_dotenv()

app = FastAPI(
    title="Guri Apartment Recommendation API",
    description="êµ¬ë¦¬ì‹œ ì•„íŒŒíŠ¸ ì¶”ì²œ AI ì„œë²„",
    version="1.0.0"
)

# -------------------------
# ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„±
# -------------------------
parser = LLMParser()
recommender = Recommender()
rag = RAGService()

# -------------------------
# ì„œë²„ ì‹œì‘ ì‹œ ì•„íŒŒíŠ¸ ë°ì´í„° ë¡œë”©
# -------------------------
APART_PATH = "backend/data/guri_apartments_final.csv"

if os.path.exists(APART_PATH):
    df_apts = pd.read_csv(APART_PATH)
    apts = df_apts.to_dict(orient="records")
    recommender.set_apartments(apts)
    print(f"ğŸ¢ ì•„íŒŒíŠ¸ {len(apts)}ê°œ ë¡œë”© ì™„ë£Œ")
else:
    print("âŒ ì•„íŒŒíŠ¸ CSV íŒŒì¼ ì—†ìŒ:", APART_PATH)


# -------------------------
# ìš”ì²­ ëª¨ë¸
# -------------------------
class Query(BaseModel):
    text: str

class RecommendRequest(BaseModel):
    conditions: dict

class SharedRequest(BaseModel):
    apt1: str
    apt2: str
    category: str = "school"
    radius: int = 800


# -------------------------
# KoBERT ëª¨ë¸ ë¡œë“œ (ìˆ˜ì •ë¨)
# -------------------------
kobert_path = "./backend/models/kobert_facility_classifier.pt"
kobert_tokenizer = get_tokenizer()
kobert_labels = ["school", "subway", "park", "hospital", "safety"]

# ì „ì—­ ë³€ìˆ˜
kobert_bert_model = None
kobert_classifier = None

if os.path.exists(kobert_path):
    try:
        print("ğŸ“¦ KoBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
        checkpoint = torch.load(kobert_path, map_location="cpu", weights_only=False)
        
        # ëª¨ë¸ ì¬êµ¬ì„±
        kobert_bert_model = get_kobert_model()
        num_labels = len(checkpoint["label_encoder"])
        kobert_classifier = nn.Linear(768, num_labels)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        kobert_bert_model.load_state_dict(checkpoint["kobert"])
        kobert_classifier.load_state_dict(checkpoint["classifier"])
        
        kobert_bert_model.eval()
        kobert_classifier.eval()
        
        # ë¼ë²¨ ì—…ë°ì´íŠ¸
        kobert_labels = checkpoint["label_encoder"].tolist()
        
        print(f"âœ… KoBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({len(kobert_labels)}ê°œ ì¹´í…Œê³ ë¦¬)")
    except Exception as e:
        print(f"âš ï¸ KoBERT ë¡œë“œ ì‹¤íŒ¨: {e}")
        kobert_bert_model = None
        kobert_classifier = None
else:
    print("â„¹ï¸ KoBERT ëª¨ë¸ íŒŒì¼ ì—†ìŒ - ê¸°ë³¸ ê¸°ëŠ¥ìœ¼ë¡œ ì‹¤í–‰")


def run_kobert(text):
    """KoBERT ì¶”ë¡ """
    if kobert_bert_model is None or kobert_classifier is None:
        return "ëª¨ë¸ ì—†ìŒ"
    
    try:
        inputs = kobert_tokenizer(
            text, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=64
        )
        
        with torch.no_grad():
            outputs = kobert_bert_model(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"]
            )[1]  # pooler_output
            logits = kobert_classifier(outputs)
        
        pred = torch.argmax(logits, dim=1).item()
        return kobert_labels[pred]
    except Exception as e:
        return f"ì¶”ë¡  ì˜¤ë¥˜: {str(e)}"


# -------------------------
# GPT-4.1
# -------------------------
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def run_gpt4(text):
    try:
        resp = openai_client.chat.completions.create(
            model="gpt-4-turbo",  # gpt-4.1ì€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ, gpt-4-turbo ì‚¬ìš©
            messages=[{"role": "user", "content": text}]
        )
        return resp.choices[0].message.content
    except Exception as e:
        return f"GPT ì˜¤ë¥˜: {str(e)}"


# -------------------------
# Claude
# -------------------------
claude_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

def run_claude(text):
    try:
        resp = claude_client.messages.create(
            model="claude-3-5-sonnet-20240620",  # ì˜¬ë°”ë¥¸ ëª¨ë¸ëª…
            max_tokens=200,
            messages=[{"role": "user", "content": text}]
        )
        return resp.content[0].text
    except Exception as e:
        return f"Claude ì˜¤ë¥˜: {str(e)}"


# -------------------------
# /predict â€“ 3ê°œ ëª¨ë¸ ë™ì‹œ í˜¸ì¶œ
# -------------------------
class PredictRequest(BaseModel):
    query: str

@app.post("/predict")
def predict(req: PredictRequest):
    text = req.query

    return {
        "kobert": run_kobert(text),
        "gpt4_1": run_gpt4(f"ì´ ë¬¸ì¥ì„ ë¶„ì„í•´ì¤˜: {text}"),
        "claude": run_claude(f"í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜: {text}")
    }


# -------------------------
# ê¸°ì¡´ ë¼ìš°í„°
# -------------------------
parse_router = APIRouter(prefix="/parse", tags=["Parser"])
recommend_router = APIRouter(prefix="/recommend", tags=["Recommendation"])
shared_router = APIRouter(prefix="/shared", tags=["Shared"])
rag_router = APIRouter(prefix="/rag", tags=["RAG"])


@parse_router.post("/")
def parse_text(req: Query):
    return {
        "input_text": req.text,
        "parsed_conditions": parser.parse_to_conditions(req.text)
    }


@recommend_router.post("/")
def recommend_api(req: RecommendRequest):
    return {
        "input_conditions": req.conditions,
        "recommendations": recommender.recommend(req.conditions)
    }


@shared_router.post("/")
def shared_api(req: SharedRequest):
    return recommender.shared_radius(
        aptA_name=req.apt1,
        aptB_name=req.apt2,
        category=req.category,
        radius=req.radius
    )


@rag_router.get("/search")
def rag_query(q: str):
    return rag.search(q)


# ë¼ìš°í„° ë“±ë¡
app.include_router(parse_router)
app.include_router(recommend_router)
app.include_router(shared_router)
app.include_router(rag_router)


@app.get("/")
def home():
    return {"message": "Guri AI Recommendation API is running"}