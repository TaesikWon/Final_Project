# backend/main.py

import os
import re
import chromadb
from fastapi import FastAPI, APIRouter
from pydantic import BaseModel
from openai import OpenAI
from pathlib import Path
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

from backend.llm_parser import LLMParser
from backend.rag.rag_service import RAGService
from backend.chat_memory import chat_memory

# ---------------------------------------------------------
# ê¸°ë³¸ ì„¤ì •
# ---------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent
load_dotenv(BASE_DIR / ".env")

app = FastAPI(
    title="Guri Apartment Recommendation API",
    description="êµ¬ë¦¬ì‹œ ì•„íŒŒíŠ¸ ì¶”ì²œ AI ì„œë²„ (ëŒ€í™”í˜• ì±—ë´‡ ëª¨ë“œ)",
    version="2.0.0",
)

from fastapi.middleware.cors import CORSMiddleware

origins = ["http://localhost:5173", "http://127.0.0.1:5173"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------------------------------------------------------
# ì„œë¹„ìŠ¤ ë¡œë”©
# ---------------------------------------------------------
parser = LLMParser()
rag = RAGService()
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
embedder = SentenceTransformer("jhgan/ko-sroberta-multitask")
chroma_client = chromadb.PersistentClient(path="./backend/rag/vector_db")

# ---------------------------------------------------------
# Request Model
# ---------------------------------------------------------
class AskRequest(BaseModel):
    question: str

# ---------------------------------------------------------
# GPT ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± (í•µì‹¬)
# ---------------------------------------------------------
def gpt_with_memory(user_question, facility_info=None, apartments=None):
    messages = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” êµ¬ë¦¬ì‹œ ì§€ì—­ì„ ì˜ ì•„ëŠ” ë¶€ë™ì‚° ì „ë¬¸ ì±—ë´‡ì´ë‹¤. "
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë§¥ë½ì„ ê¸°ì–µí•˜ë©° ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•œë‹¤."
            ),
        }
    ]

    # ìµœê·¼ 10í„´ ëŒ€í™” íˆìŠ¤í† ë¦¬
    for turn in chat_memory.history[-10:]:
        messages.append({"role": "user", "content": turn["user"]})
        messages.append({"role": "assistant", "content": turn["ai"]})

    # RAG ê²€ìƒ‰ ê²°ê³¼ë„ ì°¸ê³ ìš©ìœ¼ë¡œ ì „ë‹¬
    if facility_info or apartments:
        messages.append(
            {
                "role": "assistant",
                "content": f"ì‹œì„¤ ì •ë³´: {facility_info}\nì•„íŒŒíŠ¸ ê²€ìƒ‰ ê²°ê³¼: {apartments}",
            }
        )

    # ì´ë²ˆ ì§ˆë¬¸
    messages.append({"role": "user", "content": user_question})

    resp = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        max_tokens=2000,
        temperature=0.7,
    )

    return resp.choices[0].message.content.strip()

# ---------------------------------------------------------
# í›„ì† ì§ˆë¬¸ ì²˜ë¦¬ ("ëª‡ ê°œì•¼?", "ê°€ì¥ ê°€ê¹Œìš´ ê³³ì€?" ë“±)
# ---------------------------------------------------------
def handle_followup_question(question: str):
    """
    ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ìµœê·¼ ì¶”ì²œ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ ì²˜ë¦¬
    """
    ctx = chat_memory.get_recent_context()

    if ctx["last_recommendations"] is None:
        return None  # ì´ì „ ê²€ìƒ‰ì´ ì—†ìœ¼ë©´ í›„ì† ì§ˆë¬¸ ì•„ë‹˜

    apts = ctx["last_recommendations"]
    q = question.replace(" ", "")  # ë„ì–´ì“°ê¸° ì œê±°í•œ ë²„ì „

    # 1) ê°€ì¥ ê°€ê¹Œìš´ ì•„íŒŒíŠ¸?
    if "ê°€ê¹Œìš´" in question or "ìµœë‹¨ê±°ë¦¬" in question:
        nearest = sorted(apts, key=lambda x: x["distance_school"])[0]
        return (
            f"ê°€ì¥ ê°€ê¹Œìš´ ì•„íŒŒíŠ¸ëŠ” {nearest['apartment']}ì´ë©° "
            f"{int(nearest['distance_school'])}m ë–¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤."
        )

    # 2) ê°€ì¥ ë¨¼ ì•„íŒŒíŠ¸?
    if "ê°€ì¥ë©€" in q or "ì œì¼ë©€" in q or "ë©€ë¦¬" in question:
        far = sorted(apts, key=lambda x: x["distance_school"], reverse=True)[0]
        return (
            f"ê°€ì¥ ë¨¼ ì•„íŒŒíŠ¸ëŠ” {far['apartment']}ì´ë©° "
            f"{int(far['distance_school'])}m ë–¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤."
        )

    # 3) ê°œìˆ˜ ê´€ë ¨ ì§ˆë¬¸
    if re.search(r"ëª‡\s*ê°œ", question) or "ëª‡ê°œ" in q or "ê°œìˆ˜" in question:
        return f"ì´ {len(apts)}ê°œì˜ ì•„íŒŒíŠ¸ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤."

    return None

# ---------------------------------------------------------
# ì¶”ì²œ API (ëŒ€í™”í˜• ëª¨ë“œ)
# ---------------------------------------------------------
recommend_router = APIRouter(prefix="/recommend", tags=["Recommendation"])

@recommend_router.post("/ask")
def recommend_api(req: AskRequest):
    user_question = req.question

    # 1) ë¨¼ì € í›„ì† ì§ˆë¬¸ì¸ì§€ í™•ì¸
    followup_answer = handle_followup_question(user_question)
    if followup_answer is not None:
        # í›„ì† ì§ˆë¬¸ì´ë©´ RAG ì•ˆ ëŒë¦¬ê³  ë°”ë¡œ ë‹µë³€
        chat_memory.save_turn(user_question, followup_answer)
        return {"ok": True, "summary": followup_answer, "result": []}

    # 2) LLM íŒŒì‹±
    parsed = parser.parse(user_question)
    print("ğŸ“Œ íŒŒì‹±:", parsed)

    if parsed.get("error"):
        chat_memory.save_turn(user_question, parsed["message"])
        return {"ok": False, "error": parsed["message"], "result": []}

    mode = parsed.get("mode")
    limit = parsed.get("limit")
    apartments = []
    facility_info = None

    # 3) ì‹¤ì œ ê²€ìƒ‰ (RAG)
    if mode == "BETWEEN":
        facilities = parsed["facilities"]
        
        # ì•ˆì „í•˜ê²Œ 2ê°œë§Œ ì‚¬ìš©
        if len(facilities) < 2:
            error_msg = "ë‘ ê°œì˜ ì‹œì„¤ì´ í•„ìš”í•©ë‹ˆë‹¤."
            chat_memory.save_turn(user_question, error_msg)
            return {"ok": False, "error": error_msg, "result": []}
        
        f1 = facilities[0]
        f2 = facilities[1]
        
        apartments = rag.search_apartments_hybrid(
            parsed=parsed,
            radius=parsed.get("distance_max"),
            query=user_question,
            limit=limit,
        )

        # ì‹¤ì œ ì‹œì„¤ì˜ ì •ì‹ ëª…ì¹­ ì‚¬ìš©
        facility_info = {
            "mode": "between", 
            "f1": f1["name"],  # dictì—ì„œ name êº¼ë‚´ê¸°
            "f2": f2["name"]   # dictì—ì„œ name êº¼ë‚´ê¸°
        }

    else:
        facility_name = parsed["facility_name"]
        radius = parsed["distance_max"]

        apartments = rag.search_apartments_hybrid(
            facility_name=facility_name,
            radius=radius,
            query=user_question,
            parsed=parsed,
            limit=limit,
        )

        facility_detail = rag.search_facility_best_match(facility_name)
        address = (
            facility_detail.get("address", "ì£¼ì†Œ ì—†ìŒ") if facility_detail else "ì£¼ì†Œ ì—†ìŒ"
        )

        # ì‹¤ì œ ì‹œì„¤ì˜ ì •ì‹ ëª…ì¹­ ì‚¬ìš©
        facility_info = {
            "facility_name": facility_detail.get("name", facility_name) if facility_detail else facility_name,
            "category": parsed["facility_category"],
            "address": address,
            "radius": radius,
        }

    # 4) GPT ëŒ€í™”í˜• ì‘ë‹µ ìƒì„±
    summary = gpt_with_memory(user_question, facility_info, apartments)

    # 5) ë©”ëª¨ë¦¬ì— ì €ì¥
    chat_memory.save_recommendations(facility_info, apartments, mode)
    chat_memory.save_turn(user_question, summary)

    return {"ok": True, "summary": summary, "result": apartments}

# ---------------------------------------------------------
# ë¼ìš°í„° ë“±ë¡ & í—¬ìŠ¤ì²´í¬
# ---------------------------------------------------------
app.include_router(recommend_router)

@app.get("/")
def home():
    return {"message": "Guri AI Recommendation API running (chatbot mode)"}

@app.get("/ping")
def ping():
    return {"msg": "pong"}