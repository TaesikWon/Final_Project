# backend/main.py

import os
import torch
import torch.nn as nn
import pandas as pd

from fastapi import FastAPI, APIRouter
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import OpenAI
from anthropic import Anthropic

# =========================================
# FastAPI ìƒì„±
# =========================================
app = FastAPI(
    title="Guri Apartment Recommendation API",
    description="êµ¬ë¦¬ì‹œ ì•„íŒŒíŠ¸ ì¶”ì²œ AI ì„œë²„",
    version="1.0.0"
)

# =========================================
# CORS ì„¤ì • (âš  ë°˜ë“œì‹œ app ìƒì„± ì´í›„ì— ìœ„ì¹˜í•´ì•¼ ì •ìƒ ë™ì‘)
# =========================================
from fastapi.middleware.cors import CORSMiddleware

origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================================
# KoBERT
# =========================================
from kobert_transformers import get_tokenizer, get_kobert_model

# HuggingFace
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Custom
from backend.llm_parser import LLMParser
from backend.recommender import Recommender
from backend.rag.rag_service import RAGService
from backend.llm_explainer import explain_recommendation

# =========================================
# í™˜ê²½ ë³€ìˆ˜
# =========================================
load_dotenv()

# =========================================
# ì„œë¹„ìŠ¤ ê°ì²´
# =========================================
parser = LLMParser()
recommender = Recommender()
rag = RAGService()

# =========================================
# ì•„íŒŒíŠ¸ ë°ì´í„°
# =========================================
APART_PATH = "backend/data/apartment_guri.csv"

if os.path.exists(APART_PATH):
    df_apts = pd.read_csv(APART_PATH)
    recommender.set_apartments(df_apts.to_dict(orient="records"))
    print(f"ğŸ¢ ì•„íŒŒíŠ¸ {len(df_apts)}ê°œ ë¡œë”© ì™„ë£Œ")
else:
    print("âŒ ì•„íŒŒíŠ¸ CSV íŒŒì¼ ì—†ìŒ:", APART_PATH)

# =========================================
# Request Models
# =========================================
class Query(BaseModel):
    text: str

class RecommendRequest(BaseModel):
    conditions: dict

class SharedRequest(BaseModel):
    apt1: str
    apt2: str
    category: str = "school"
    radius: int = 800

class PredictRequest(BaseModel):
    text: str

# =========================================
# ê³µí†µ ë¼ë²¨
# =========================================
LABELS = ["sports", "shopping", "hospital", "market", "restaurant", "school", "cafe"]
NUM_LABELS = len(LABELS)

# =========================================
# KoBERT ë¡œë“œ
# =========================================
kobert_path = "./backend/models/kobert_facility_classifier.pt"

kobert_tokenizer = get_tokenizer()
kobert_bert_model = None
kobert_classifier = None

if os.path.exists(kobert_path):
    try:
        print("ğŸ“¦ KoBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
        checkpoint = torch.load(kobert_path, map_location="cpu")

        kobert_bert_model = get_kobert_model()
        kobert_classifier = nn.Linear(768, NUM_LABELS)

        kobert_bert_model.load_state_dict(checkpoint["kobert"])
        kobert_classifier.load_state_dict(checkpoint["classifier"])

        kobert_bert_model.eval()
        kobert_classifier.eval()

        print("âœ… KoBERT ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        print("âš  KoBERT ë¡œë“œ ì‹¤íŒ¨:", e)
else:
    print("â„¹ KoBERT ëª¨ë¸ ì—†ìŒ")


def run_kobert(text):
    if kobert_bert_model is None:
        return "KoBERT ëª¨ë¸ ì—†ìŒ"

    try:
        inputs = kobert_tokenizer(text, return_tensors="pt", padding=True, truncation=True)

        with torch.no_grad():
            _, pooled = kobert_bert_model(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                return_dict=False
            )
            logits = kobert_classifier(pooled)

        pred = torch.argmax(logits, dim=1).item()
        return LABELS[pred]

    except Exception as e:
        return f"KoBERT ì˜¤ë¥˜: {e}"


# =========================================
# KLUE
# =========================================
try:
    print("ğŸ“˜ KLUE ë¡œë”© ì¤‘â€¦")
    klue_tokenizer = AutoTokenizer.from_pretrained("klue/roberta-small")
    klue_model = AutoModelForSequenceClassification.from_pretrained(
        "klue/roberta-small",
        num_labels=NUM_LABELS
    )
    klue_model.load_state_dict(torch.load("./backend/models/klue_facility_classifier.pt"))
    klue_model.eval()
    print("âœ… KLUE ë¡œë”© ì™„ë£Œ")
except:
    klue_model = None
    print("âš  KLUE ë¡œë“œ ì‹¤íŒ¨")


def run_klue(text):
    if klue_model is None:
        return "KLUE ëª¨ë¸ ì—†ìŒ"

    try:
        inputs = klue_tokenizer(text, return_tensors="pt", padding=True, truncation=True)

        with torch.no_grad():
            logits = klue_model(**inputs).logits

        pred = torch.argmax(logits, dim=1).item()
        return LABELS[pred]
    except Exception as e:
        return f"KLUE ì˜¤ë¥˜: {e}"


# =========================================
# ELECTRA
# =========================================
try:
    print("ğŸŸ© ELECTRA ë¡œë”© ì¤‘â€¦")
    electra_tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-small-v3-discriminator")
    electra_model = AutoModelForSequenceClassification.from_pretrained(
        "monologg/koelectra-small-v3-discriminator",
        num_labels=NUM_LABELS
    )
    electra_model.load_state_dict(torch.load("./backend/models/electra_facility_classifier.pt"))
    electra_model.eval()
    print("âœ… ELECTRA ë¡œë”© ì™„ë£Œ")
except:
    electra_model = None
    print("âš  ELECTRA ë¡œë“œ ì‹¤íŒ¨")


def run_electra(text):
    if electra_model is None:
        return "ELECTRA ëª¨ë¸ ì—†ìŒ"

    try:
        inputs = electra_tokenizer(text, return_tensors="pt", padding=True, truncation=True)

        with torch.no_grad():
            logits = electra_model(**inputs).logits

        pred = torch.argmax(logits, dim=1).item()
        return LABELS[pred]
    except Exception as e:
        return f"ELECTRA ì˜¤ë¥˜: {e}"


# =========================================
# GPT / Claude
# =========================================
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
claude_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

# =========================================
# Router ë“±ë¡
# =========================================
parse_router = APIRouter(prefix="/parse", tags=["Parser"])
recommend_router = APIRouter(prefix="/recommend", tags=["Recommendation"])
shared_router = APIRouter(prefix="/shared", tags=["Shared"])
rag_router = APIRouter(prefix="/rag", tags=["RAG"])


@parse_router.post("/")
def parse_text(req: Query):
    return parser.parse_to_conditions(req.text)


@recommend_router.post("/")
def recommend(req: RecommendRequest):
    return recommender.recommend(req.conditions)


@rag_router.get("/search")
def rag_search(q: str):
    return rag.search(q)


@shared_router.post("/")
def shared_info(req: SharedRequest):
    return recommender.compare_shared(req.apt1, req.apt2, req.category, req.radius)


# ğŸ‘ ëª¨ë¸ ë¹„êµ
@app.post("/predict_models")
def predict_models(req: PredictRequest):
    t = req.text
    return {
        "input": t,
        "KoBERT": run_kobert(t),
        "KLUE": run_klue(t),
        "ELECTRA": run_electra(t)
    }


# ë¼ìš°í„° ë“±ë¡
app.include_router(parse_router)
app.include_router(recommend_router)
app.include_router(shared_router)
app.include_router(rag_router)


@app.get("/")
def home():
    return {"message": "Guri AI Recommendation API is running"}
